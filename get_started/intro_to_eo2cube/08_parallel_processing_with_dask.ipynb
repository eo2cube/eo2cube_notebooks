{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"../../additional_data/banner_siegel.png\" style=\"width:1100px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processing with Dask\n",
    "\n",
    "* [**Sign up to the JupyterHub**](https://www.phenocube.org/) to run this notebook interactively from your browser\n",
    "* **Compatibility:** Notebook currently compatible with the Open Data Cube environments of the University of Wuerzburg\n",
    "* **Products used**: \n",
    "* **Prerequisites**:  Users of this notebook should have a basic understanding of:\n",
    "    * How to run a [Jupyter notebook](01_jupyter_introduction.ipynb)\n",
    "    * The basic structure of the eo2cube [satellite datasets](02_eo2cube.ipynb)\n",
    "    * How to browse through the available [products and measurements](03_products_and_measurements.ipynb) of the eo2cube datacube \n",
    "    * How to [load data from the eo2cube datacube](04_loading_data_and_basic_xarray.ipynb)\n",
    "    * How the data is stored and structured in a [xarray](05_advanced_xarray.ipynb)\n",
    "    * How to [visualize the data](06_plotting.ipynb)\n",
    "    * How to do a [basic analysis of remote sensing data](07_basic_analysis.ipynb) in the eo2cube environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dask?\n",
    "[Dask](https://dask.org/) is a library for parallel computing. \n",
    "\n",
    "Single-machine Scheduler vs. Distributed scheduler\n",
    "Distributed Scheduler provides more features and diagnostics, suggested to use (even for single machine applications)\n",
    "<br>\n",
    "(https://docs.dask.org/en/latest/setup.html) \n",
    "<br>\n",
    "two functionalities:\n",
    "1. \"Dynamic task scheduling optimized for computation\"\n",
    "2. \"\"Big Data\" collections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description / Structure of this Notebook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package import and datacube connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all you start with importing all the packages that are needed for our analysis.<br>\n",
    "Most of the packages have already been introduced in the previous notebooks. The newly introduced package here is Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "\n",
    "# dask\n",
    "import dask\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connecting to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app = '08_parallel_processing_with_dask', config = '/home/datacube/.datacube.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Dask.distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not working\n",
    "client=Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data using dask\n",
    "In the following, the data is loaded.\n",
    "As you remeber from the previous notebooks a \"normal\" command for loading data looks somewhat like this: <br>\n",
    "```python\n",
    "ds = dc.load(product = \"s2_l2a_bavaria\",\n",
    "             measurements = [\"blue\", \"green\",\"red\", \"red_edge2\", \"nir\", \"narrow_nir\"],\n",
    "             longitude = [12.493, 12.509],\n",
    "             latitude = [47.861, 47.868],\n",
    "             time = (\"2020-04-01\", \"2021-03-31\"))\n",
    "```\n",
    "For loading data with dask, you just add the \"dask_chunks\"-parameter. <br>\n",
    "This looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "ds = dc.load(product = \"s2_l2a_bavaria\",\n",
    "             measurements = [\"blue\", \"green\",\"red\", \"red_edge2\", \"nir\", \"narrow_nir\"],\n",
    "             longitude = [12.493, 12.509],\n",
    "             latitude = [47.861, 47.868],\n",
    "             time = (\"2020-04-01\", \"2021-03-31\"),\n",
    "             dask_chunks={\"time\": 1, \"x\": 50, \"y\": 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what we have done here, lets first look at the dataset we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the xarray.Dataset consists of multiple dask.array objects but we cannot see any vlaues inside our data. <br>\n",
    "This type of data is called \"Lazy data\" as it is not loaded properly but in a \"lazy\" way without data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Chunks\n",
    "The parameter that was added to the dc.load() command is called \"dask_chunks\". It defines in how many parts our original dataset will be splitted. As described in the previous notebooks, normally the dc.load() command produces a xarray dataset(??).<br>\n",
    "In our case, the dataset is split into smaller chunks. Since the data we are interested in is three dimensional, we also need to provide three dimensions for subdividing the data. <br>\n",
    "The provided values have been {\"time\": 10, \"x\": 50, \"y\": 50}. Accordingly, the chunksize of the dask.arrays is (10,50,50). <br>\n",
    "<br>\n",
    "In the following we will visualize the lazy-loaded data for the red band to get an even better feeling about our type of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the dask chunks\n",
    "ds.red.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data has been devided into a total of 216 chunks, each having a size of 10 timesteps, 50 pixels in x-direction, and 50 pixels in y-direction. <br>\n",
    "Looking at the memory size of the chunks compared to the complete array, the motivation for using dask becomes clear. Especially when working with large amounts of data, splitting the data into smaller chunks enables computations that would crash the Phenocube environment when calculated over the complete array at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to do a computation on lazy data, it makes sense to chain operations together and to just calculate the values right at the end (with the load() function). \n",
    "One example for that is the following calculation of NDVI values on the lazy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_diff = ds.nir - ds.red\n",
    "band_sum = ds.nir + ds.red\n",
    "\n",
    "ds[\"ndvi\"] = band_diff / band_sum\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now calculated the NDVI based on lazy-loaded data. Since we have not used the load() function, the ndvi itself is a lazy-loaded dask.array as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of dask is its ability to only perform necessary operations within a process. (Here there is still some information missing!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work! graphviz python library & graphviz system library need to be installed\n",
    "# ds.red.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following functions from:\n",
    "https://github.com/Shirobakaidou/eo2cube_notebooks/blob/main/get_started/intro_to_eo2cube/07_ts_tesselCap.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month, Mean\n",
    "ds_mmean = ds.groupby('time.month').mean(dim='time')\n",
    "print(ds_mmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesseled Cap Wetness\n",
    "wet = 0.1509*ds_mmean.blue + 0.1973*ds_mmean.green + 0.3279*ds_mmean.red + 0.3406*ds_mmean.nir-0.711211-0.457212\n",
    "# Tesseled Cap Green Vegetation\n",
    "gvi = -0.2848*ds_mmean.blue-0.2435*ds_mmean.green-0.5436*ds_mmean.red + 0.7243*ds_mmean.nir + 0.084011-0.180012\n",
    "# Tesseled Cap Soil Brightness\n",
    "sbi = 0.332*ds_mmean.green + 0.603*ds_mmean.red + 0.675*ds_mmean.red_edge2 + 0.262*ds_mmean.narrow_nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mmean['wet']=wet\n",
    "ds_mmean['gvi']=gvi\n",
    "ds_mmean['sbi']=sbi\n",
    "print(ds_mmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mmean.red.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_mmean.gvi.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the lazy data\n",
    "Of course it is possible to convert lazy data into \"normal\" data. This can be done using the .load() command. <br>\n",
    "The resulting data has the same format as data loaded without dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing:\n",
    "# compute() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue with the beginner's guide, the following notebooks are designed to be worked through in the following order:\n",
    "\n",
    "1. [Jupyter Notebooks](01_jupyter_introduction.ipynb)\n",
    "2. [eo2cube](02_eo2cube.ipynb)\n",
    "3. [Products and Measurements](03_products_and_measurements.ipynb)\n",
    "4. [Loading data and introduction to xarrays](04_loading_data_and_basic_xarray.ipynb)\n",
    "5. [Advanced xarrays operations](05_advanced_xarray.ipynb)\n",
    "6. [Plotting data](06_plotting.ipynb)\n",
    "7. [Basic analysis of remote sensing data](07_basic_analysis.ipynb)\n",
    "8. **Parallel processing with Dask (this notebook)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "<font size=\"2\">This notebook for the usage in the Open Data Cube entities of the [Department of Remote Sensing](http://remote-sensing.org/), [University of Wuerzburg](https://www.uni-wuerzburg.de/startseite/), is adapted from [Geoscience Australia](https://github.com/GeoscienceAustralia/dea-notebooks), published using the Apache License, Version 2.0. Thanks!</font>\n",
    "\n",
    "https://doi.org/10.26186/145234 <br>\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "\n",
    "**Contact:** If you would like to report an issue with this notebook, you can file one on [Github](https://github.com).\n",
    "\n",
    "**Last modified:** February 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
